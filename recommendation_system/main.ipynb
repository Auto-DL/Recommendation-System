{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.1 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "75dbba10fc6620c626cd1fa2436ff69a4b43b4d8107ad1b0d4ae9140f5dc976d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import github as gh\n",
    "import bs4 as bs\n",
    "import requests\n",
    "from lxml import etree\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AuthenticatedUser(login=None)\n"
     ]
    }
   ],
   "source": [
    "GITHUB_ACCESS_TOKEN=os.getenv('GITHUB_ACCESS_TOKEN')\n",
    "g=gh.Github(GITHUB_ACCESS_TOKEN)\n",
    "print(g.get_user())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "29\n",
      "['_PaginatedListBase__elements', '_PaginatedListBase__fetchToIndex', '_PaginatedList__contentClass', '_PaginatedList__firstParams', '_PaginatedList__firstUrl', '_PaginatedList__headers', '_PaginatedList__list_item', '_PaginatedList__nextParams', '_PaginatedList__nextUrl', '_PaginatedList__parseLinkHeader', '_PaginatedList__requester', '_PaginatedList__reverse', '_PaginatedList__totalCount', '_Slice', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_couldGrow', '_fetchNextPage', '_getLastPageUrl', '_grow', '_isBiggerThan', '_reversed', 'get_page', 'reversed', 'totalCount']\n",
      "https://github.com/Z-yq/TensorflowTTS\n",
      "https://github.com/Kal213/StarGAN-Tutorial-Tensorflow-2.3\n",
      "https://github.com/lshug/first-order-model-tf\n",
      "https://github.com/veeenie/Tensorflow2\n",
      "https://github.com/hubertkarbowy/awd_lstm_tensorflow2\n",
      "https://github.com/boa50/tensorflow-styletransfer\n",
      "https://github.com/sk981102/tensorflow_certificate\n",
      "https://github.com/Zwyywz/TensorFlowLearningNotes\n",
      "https://github.com/Developik/tensorflow_image_model\n",
      "https://github.com/boa50/tensorflow-pix2pix\n",
      "https://github.com/huww98/torch-tensorflow-wrapper\n",
      "https://github.com/fadhil-code/TensorFlowProject3\n",
      "https://github.com/danilyef/ResNet-Tensorflow\n",
      "https://github.com/aysedemirel/TensorFlow-Pokemon-Course\n",
      "https://github.com/Astro-Astre/tensorflow_merger_classifier\n",
      "https://github.com/junyuchen245/Semi-supervised_FCM_Loss_for_Segmentation\n",
      "https://github.com/artset/image-to-illustration\n",
      "https://github.com/Vezqi/tf-emotes\n",
      "https://github.com/wchen777/ASL-recog\n",
      "https://github.com/The-bot-makers/Neural-Networks\n",
      "https://github.com/Moulishankar10/Employee-Salary-Classification\n",
      "https://github.com/AnkitaVyas77/AccidentDetection_SSD-Mobilenet\n",
      "https://github.com/stoguri11/SentAnalysis\n",
      "https://github.com/stevenohohohoh/Neural-Style-Transfer\n",
      "https://github.com/mhnaeem/emotion-detection\n",
      "https://github.com/nateyoungblood/rnn_titanic\n",
      "https://github.com/Tyler-Shamsuddoha/python-image-classifier-keras\n",
      "https://github.com/MalayAgr/MesoNet-DeepfakeDetection-API\n",
      "https://github.com/sidphbot/AutoKeras-ArcFaceHead\n"
     ]
    }
   ],
   "source": [
    "query='tensorflow language:python created:2021-04-01..2021-04-02'\n",
    "urls = []\n",
    "result=g.search_repositories(query)\n",
    "print(result.totalCount)\n",
    "print(dir(result))\n",
    "for repository in result:\n",
    "    urls.append(repository.html_url)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://github.com/Tyler-Shamsuddoha/python-image-classifier-keras'\n",
    "url2='https://github.com/bamblebam/image-classification-rps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 91.0.4472\n",
      "Get LATEST driver version for 91.0.4472\n",
      "Get LATEST driver version for 91.0.4472\n",
      "Trying to download new driver from https://chromedriver.storage.googleapis.com/91.0.4472.101/chromedriver_win32.zip\n",
      "Driver has been saved in cache [./\\drivers\\chromedriver\\win32\\91.0.4472.101]\n"
     ]
    }
   ],
   "source": [
    "options=Options()\n",
    "options.headless=True\n",
    "driver=webdriver.Chrome(ChromeDriverManager(path='./').install(),options=options)\n",
    "driver.get(url2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "table=driver.find_element_by_xpath(\"//*[@class='Details-content--hidden-not-important js-navigation-container js-active-navigation-container d-md-block']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# links=table.find_elements_by_tag_name('a')\n",
    "# print(len(links))\n",
    "# for link in links:\n",
    "#     href=link.get_attribute('href')\n",
    "#     if 'tree' in href or '.py' in href:\n",
    "#         stack.append(href)\n",
    "\n",
    "# driver.get(url2)\n",
    "# table=driver.find_element_by_xpath(\"//*[@class='Details-content--hidden-not-important js-navigation-container js-active-navigation-container d-md-block']\")\n",
    "# links=table.find_elements_by_tag_name('a')\n",
    "# for link in links:\n",
    "#     href=link.get_attribute('href')\n",
    "#     if '/tree/' in href or '.py' in href:\n",
    "#         stack.append(href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue=list()\n",
    "full_list=list()\n",
    "links_list = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_to_queue(links):\n",
    "    for link in links:\n",
    "        href=link.get_attribute('href')\n",
    "        if href in links_list:\n",
    "            continue\n",
    "        if '/tree/' in href or '.py' in href:\n",
    "            links_list.append(href)\n",
    "            queue.append(href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_through_files(link):\n",
    "    if '/tree/' in link:\n",
    "        driver.get(link)\n",
    "        time.sleep(2.5)\n",
    "        table=driver.find_element_by_xpath(\"//*[@class='Details-content--hidden-not-important js-navigation-container js-active-navigation-container d-block']\")\n",
    "        links=table.find_elements_by_tag_name('a')\n",
    "        push_to_queue(links)\n",
    "    elif '.py' in link:\n",
    "        driver.get(link)\n",
    "        time.sleep(2.5)\n",
    "        full_list.append(link)\n",
    "        #preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs():\n",
    "    while queue:\n",
    "        link=queue.pop(0)\n",
    "        print(link)\n",
    "        search_through_files(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_relevant_links(url):\n",
    "    links_list.append(url)\n",
    "    driver.get(url)\n",
    "    table=driver.find_element_by_xpath(\"//*[@class='Details-content--hidden-not-important js-navigation-container js-active-navigation-container d-md-block']\")\n",
    "    links=table.find_elements_by_tag_name('a')\n",
    "    for link in links:\n",
    "        href=link.get_attribute('href')\n",
    "        if '/tree/' in href or '.py' in href:\n",
    "            links_list.append(href)\n",
    "            queue.append(href)\n",
    "    bfs()\n",
    "    print(\"Ended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "https://github.com/Z-yq/TensorflowTTS/tree/main/.idea\n",
      "https://github.com/Z-yq/TensorflowTTS/tree/main/configs\n",
      "https://github.com/Z-yq/TensorflowTTS/tree/main/dataloaders\n",
      "https://github.com/Z-yq/TensorflowTTS/tree/main/dump\n",
      "https://github.com/Z-yq/TensorflowTTS/tree/main/models\n",
      "https://github.com/Z-yq/TensorflowTTS/tree/main/synthesizer\n",
      "https://github.com/Z-yq/TensorflowTTS/tree/main/trainer\n",
      "https://github.com/Z-yq/TensorflowTTS/tree/main/utils\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/fastspeech_extract_features.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/run-test.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/tacotron_extract_features.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/train_acoustic.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/train_vocoder.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/dataloaders/fastspeech_dataloader.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/dataloaders/tacotron_dataloader.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/dataloaders/vocoder_dataloader.py\n",
      "https://github.com/Z-yq/TensorflowTTS/tree/main/models/layers\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/models/__init__.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/models/conformer.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/models/fastspeech.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/models/model.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/models/positional_encoding.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/models/switchnorm.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/models/tacotron2.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/models/vocoder.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/models/weight_norm.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/synthesizer/synthesizer_fastspeech.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/synthesizer/synthesizer_tacotron.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/trainer/base_runners.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/trainer/fastspeech_trainer.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/trainer/tacotron_trainer.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/trainer/vocoder_trainer.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/utils/decoder.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/utils/normalize.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/utils/plot.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/utils/speech_featurizers.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/utils/stft.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/utils/text_featurizers.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/utils/tools.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/utils/user_config.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/utils/utils.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/LayerNormLstmCell.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/__init__.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/backend.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/backend_keras.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/decoder.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/filterbank.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/frame_wise_batch_norm_lstm_cell.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/merge_two_last_dims.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/multihead_attention.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/multihead_self_attention.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/point_wise_ffn.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/positional_encoding.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/row_conv_1d.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/sequence_wise_batch_norm.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/switchnorm.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/time_frequency.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/time_reduction.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/transpose_time_major.py\n",
      "https://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/utils.py\n",
      "Ended\n"
     ]
    }
   ],
   "source": [
    "res=get_all_relevant_links('https://github.com/Z-yq/TensorflowTTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "https://github.com/Z-yq/TensorflowTTS/blob/main/fastspeech_extract_features.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/run-test.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/tacotron_extract_features.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/train_acoustic.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/train_vocoder.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/dataloaders/fastspeech_dataloader.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/dataloaders/tacotron_dataloader.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/dataloaders/vocoder_dataloader.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/models/__init__.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/models/conformer.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/models/fastspeech.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/models/model.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/models/positional_encoding.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/models/switchnorm.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/models/tacotron2.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/models/vocoder.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/models/weight_norm.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/synthesizer/synthesizer_fastspeech.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/synthesizer/synthesizer_tacotron.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/trainer/base_runners.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/trainer/fastspeech_trainer.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/trainer/tacotron_trainer.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/trainer/vocoder_trainer.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/utils/decoder.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/utils/normalize.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/utils/plot.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/utils/speech_featurizers.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/utils/stft.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/utils/text_featurizers.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/utils/tools.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/utils/user_config.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/utils/utils.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/LayerNormLstmCell.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/__init__.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/backend.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/backend_keras.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/decoder.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/filterbank.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/frame_wise_batch_norm_lstm_cell.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/merge_two_last_dims.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/multihead_attention.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/multihead_self_attention.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/point_wise_ffn.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/positional_encoding.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/row_conv_1d.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/sequence_wise_batch_norm.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/switchnorm.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/time_frequency.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/time_reduction.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/transpose_time_major.py\nhttps://github.com/Z-yq/TensorflowTTS/blob/main/models/layers/utils.py\n"
     ]
    }
   ],
   "source": [
    "for i in full_list:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "51 51\n"
     ]
    }
   ],
   "source": [
    "list_set=set(full_list)\n",
    "print(len(full_list),len(list_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSequentialModel():\n",
    "    for url in full_list:\n",
    "        print()\n",
    "        driver.get(url)\n",
    "        table=driver.find_element_by_xpath(\"//*[@class='Box-body p-0 blob-wrapper data type-python  gist-border-0']\")\n",
    "        if 'Sequential' in table.text:\n",
    "            return table.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "       context_layer = tf.transpose(context_layer, perm=[0, 2, 1, 3])\n",
      "        context_layer = tf.reshape(\n",
      "            context_layer, (batch_size, -1, self.all_head_size)\n",
      "        )\n",
      "        outputs = (context_layer, attention_probs) if self.output_attentions else (context_layer,)\n",
      "        return outputs\n",
      "class TFFastSpeechSelfOutput(tf.keras.layers.Layer):\n",
      "    \"\"\"Fastspeech output of self attention module.\"\"\"\n",
      "    def __init__(self, config, **kwargs):\n",
      "        \"\"\"Init variables.\"\"\"\n",
      "        super().__init__(**kwargs)\n",
      "        self.dense = tf.keras.layers.Dense(\n",
      "            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n",
      "        )\n",
      "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n",
      "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
      "    def call(self, inputs, training=False):\n",
      "        \"\"\"Call logic.\"\"\"\n",
      "        hidden_states, input_tensor = inputs\n",
      "        hidden_states = self.dense(hidden_states)\n",
      "        hidden_states = self.dropout(hidden_states, training=training)\n",
      "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "        return hidden_states\n",
      "class TFFastSpeechAttention(tf.keras.layers.Layer):\n",
      "    \"\"\"Fastspeech attention module.\"\"\"\n",
      "    def __init__(self, config, **kwargs):\n",
      "        \"\"\"Init variables.\"\"\"\n",
      "        super().__init__(**kwargs)\n",
      "        self.self_attention = TFFastSpeechSelfAttention(config, name=\"self\")\n",
      "        self.dense_output = TFFastSpeechSelfOutput(config, name=\"output\")\n",
      "    def call(self, inputs, training=False):\n",
      "        input_tensor, attention_mask = inputs\n",
      "        self_outputs = self.self_attention([input_tensor, attention_mask], training=training)\n",
      "        attention_output = self.dense_output([self_outputs[0], input_tensor], training=training)\n",
      "        masked_attention_output = attention_output * tf.cast(tf.expand_dims(attention_mask, 2), dtype=tf.float32)\n",
      "        outputs = (masked_attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
      "        return outputs\n",
      "class TFFastSpeechIntermediate(tf.keras.layers.Layer):\n",
      "    \"\"\"Intermediate representation module.\"\"\"\n",
      "    def __init__(self, config, **kwargs):\n",
      "        \"\"\"Init variables.\"\"\"\n",
      "        super().__init__(**kwargs)\n",
      "        self.conv1d_1 = tf.keras.layers.Conv1D(\n",
      "            config.intermediate_size,\n",
      "            kernel_size=config.intermediate_kernel_size,\n",
      "            kernel_initializer=get_initializer(config.initializer_range),\n",
      "            padding='same',\n",
      "            name=\"conv1d_1\"\n",
      "        )\n",
      "        self.conv1d_2 = tf.keras.layers.Conv1D(\n",
      "            config.hidden_size,\n",
      "            kernel_size=config.intermediate_kernel_size,\n",
      "            kernel_initializer=get_initializer(config.initializer_range),\n",
      "            padding='same',\n",
      "            name=\"conv1d_2\"\n",
      "        )\n",
      "        if isinstance(config.hidden_act, str):\n",
      "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
      "        else:\n",
      "            self.intermediate_act_fn = config.hidden_act\n",
      "    def call(self, inputs):\n",
      "        \"\"\"Call logic.\"\"\"\n",
      "        hidden_states, attention_mask = inputs\n",
      "        hidden_states = self.conv1d_1(hidden_states)\n",
      "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
      "        hidden_states = self.conv1d_2(hidden_states)\n",
      "        masked_hidden_states = hidden_states * tf.cast(tf.expand_dims(attention_mask, 2), dtype=tf.float32)\n",
      "        return masked_hidden_states\n",
      "class TFFastSpeechOutput(tf.keras.layers.Layer):\n",
      "    \"\"\"Output module.\"\"\"\n",
      "    def __init__(self, config, **kwargs):\n",
      "        \"\"\"Init variables.\"\"\"\n",
      "        super().__init__(**kwargs)\n",
      "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n",
      "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
      "    def call(self, inputs, training=False):\n",
      "        \"\"\"Call logic.\"\"\"\n",
      "        hidden_states, input_tensor = inputs\n",
      "        hidden_states = self.dropout(hidden_states, training=training)\n",
      "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "        return hidden_states\n",
      "class TFFastSpeechLayer(tf.keras.layers.Layer):\n",
      "    \"\"\"Fastspeech module (FFT module on the paper).\"\"\"\n",
      "    def __init__(self, config, **kwargs):\n",
      "        \"\"\"Init variables.\"\"\"\n",
      "        super().__init__(**kwargs)\n",
      "        self.attention = TFFastSpeechAttention(config, name=\"attention\")\n",
      "        self.intermediate = TFFastSpeechIntermediate(config, name=\"intermediate\")\n",
      "        self.bert_output = TFFastSpeechOutput(config, name=\"output\")\n",
      "    def call(self, inputs, training=False):\n",
      "        \"\"\"Call logic.\"\"\"\n",
      "        hidden_states, attention_mask = inputs\n",
      "        attention_outputs = self.attention([hidden_states, attention_mask], training=training)\n",
      "        attention_output = attention_outputs[0]\n",
      "        intermediate_output = self.intermediate([attention_output, attention_mask], training=training)\n",
      "        layer_output = self.bert_output([intermediate_output, attention_output], training=training)\n",
      "        masked_layer_output = layer_output * tf.cast(tf.expand_dims(attention_mask, 2), dtype=tf.float32)\n",
      "        outputs = (masked_layer_output,) + attention_outputs[1:]  # add attentions if we output them\n",
      "        return outputs\n",
      "class TFFastSpeechEncoder(tf.keras.layers.Layer):\n",
      "    \"\"\"Fast Speech encoder module.\"\"\"\n",
      "    def __init__(self, config, **kwargs):\n",
      "        \"\"\"Init variables.\"\"\"\n",
      "        super().__init__(**kwargs)\n",
      "        self.output_attentions = config.output_attentions\n",
      "        self.output_hidden_states = config.output_hidden_states\n",
      "        self.layer = [TFFastSpeechLayer(config, name=\"layer_._{}\".format(i)) for i in range(config.num_hidden_layers)]\n",
      "    def call(self, inputs, training=False):\n",
      "        \"\"\"Call logic.\"\"\"\n",
      "        hidden_states, attention_mask = inputs\n",
      "        all_hidden_states = ()\n",
      "        all_attentions = ()\n",
      "        for _, layer_module in enumerate(self.layer):\n",
      "            if self.output_hidden_states:\n",
      "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
      "            layer_outputs = layer_module([hidden_states, attention_mask], training=training)\n",
      "            hidden_states = layer_outputs[0]\n",
      "            if self.output_attentions:\n",
      "                all_attentions = all_attentions + (layer_outputs[1],)\n",
      "        # Add last layer\n",
      "        if self.output_hidden_states:\n",
      "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
      "        outputs = (hidden_states,)\n",
      "        if self.output_hidden_states:\n",
      "            outputs = outputs + (all_hidden_states,)\n",
      "        if self.output_attentions:\n",
      "            outputs = outputs + (all_attentions,)\n",
      "        return outputs  # outputs, (hidden states), (attentions)\n",
      "class TFFastSpeechDecoder(TFFastSpeechEncoder):\n",
      "    \"\"\"Fast Speech decoder module.\"\"\"\n",
      "    def __init__(self, config, **kwargs):\n",
      "        super().__init__(config, **kwargs)\n",
      "        self.config = config\n",
      "        # create decoder positional embedding\n",
      "        self.decoder_positional_embeddings = tf.keras.layers.Embedding(\n",
      "            config.max_position_embeddings + 1,\n",
      "            config.hidden_size,\n",
      "            weights=[self._sincos_embedding()],\n",
      "            name=\"position_embeddings\",\n",
      "            trainable=False\n",
      "        )\n",
      "        if config.n_speakers > 1:\n",
      "            self.speaker_ex=tf.keras.layers.Dense(config.hidden_size,name='speaker_explain')\n",
      "    def call(self, inputs, training=False):\n",
      "        hidden_states, speaker_info, encoder_mask, decoder_pos = inputs\n",
      "        # calculate new hidden states.\n",
      "        hidden_states = hidden_states + self.decoder_positional_embeddings(decoder_pos)\n",
      "        if self.config.n_speakers > 1:\n",
      "            length=tf.shape(hidden_states)[1]\n",
      "            extended_speaker_features=tf.repeat(speaker_info,length,1)\n",
      "            hidden_states = tf.concat([hidden_states,extended_speaker_features],-1)\n",
      "            hidden_states=self.speaker_ex(hidden_states)\n",
      "        return super().call([hidden_states, encoder_mask], training=training)\n",
      "    def _sincos_embedding(self):\n",
      "        position_enc = np.array([\n",
      "            [pos / np.power(10000, 2.0 * (i // 2) / self.config.hidden_size) for i in range(self.config.hidden_size)]\n",
      "            for pos in range(self.config.max_position_embeddings + 1)\n",
      "        ])\n",
      "        position_enc[:, 0::2] = np.sin(position_enc[:, 0::2])\n",
      "        position_enc[:, 1::2] = np.cos(position_enc[:, 1::2])\n",
      "        # pad embedding.\n",
      "        position_enc[0] = 0.0\n",
      "        return position_enc\n",
      "class TFPostnet(tf.keras.layers.Layer):\n",
      "    def __init__(self, config, **kwargs):\n",
      "        \"\"\"Init variables.\"\"\"\n",
      "        super().__init__(**kwargs)\n",
      "        self.conv_batch_norm = []\n",
      "        for i in range(config.n_conv_postnet):\n",
      "            conv = tf.keras.layers.Conv1D(\n",
      "                filters=config.postnet_conv_filters,\n",
      "                kernel_size=config.postnet_conv_kernel_sizes,\n",
      "                padding='same',\n",
      "                name='conv_._{}'.format(i)\n",
      "            )\n",
      "            batch_norm = SwitchNormalization(name='switch_norm_._{}'.format(i))\n",
      "            self.conv_batch_norm.append((conv, batch_norm))\n",
      "        self.dropout = tf.keras.layers.Dropout(rate=config.postnet_dropout_rate, name='dropout')\n",
      "        self.activation = [tf.nn.leaky_relu] * (config.n_conv_postnet)\n",
      "        self.final=tf.keras.layers.Dense(config.num_mels)\n",
      "    def call(self, inputs, training=False):\n",
      "        \"\"\"Call logic.\"\"\"\n",
      "        outputs, mask = inputs\n",
      "        extended_mask = tf.cast(tf.expand_dims(mask, axis=2), tf.float32)\n",
      "        for i, (conv, bn) in enumerate(self.conv_batch_norm):\n",
      "            outputs = conv(outputs)\n",
      "            outputs = bn(outputs)\n",
      "            outputs = self.activation[i](outputs)\n",
      "            outputs = self.dropout(outputs, training=training)\n",
      "        outputs=self.final(outputs)\n",
      "        outputs=outputs\n",
      "        return outputs * extended_mask\n",
      "class TFFastSpeechDurationPredictor(tf.keras.layers.Layer):\n",
      "    \"\"\"FastSpeech duration predictor module.\"\"\"\n",
      "    def __init__(self, config, **kwargs):\n",
      "        \"\"\"Init variables.\"\"\"\n",
      "        super().__init__(**kwargs)\n",
      "        self.conv_layers = []\n",
      "        for i in range(config.num_duration_conv_layers):\n",
      "            self.conv_layers.append(\n",
      "                tf.keras.layers.Conv1D(\n",
      "                    config.duration_predictor_filters,\n",
      "                    config.duration_predictor_kernel_sizes,\n",
      "                    padding='same',\n",
      "                    name='conv_._{}'.format(i)\n",
      "                )\n",
      "            )\n",
      "            self.conv_layers.append(\n",
      "                tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm_._{}\".format(i))\n",
      "            )\n",
      "            self.conv_layers.append(\n",
      "                tf.keras.layers.Activation(tf.nn.relu6)\n",
      "            )\n",
      "            self.conv_layers.append(\n",
      "                tf.keras.layers.Dropout(config.duration_predictor_dropout_probs)\n",
      "            )\n",
      "        self.conv_layers_sequence = tf.keras.Sequential(self.conv_layers)\n",
      "        self.output_layer = tf.keras.layers.Dense(1)\n",
      "    def call(self, inputs, training=False):\n",
      "        \"\"\"Call logic.\"\"\"\n",
      "        encoder_hidden_states, attention_mask,spk_info = inputs\n",
      "        attention_mask = tf.cast(tf.expand_dims(attention_mask, 2), tf.float32)\n",
      "        # mask encoder hidden states\n",
      "        masked_encoder_hidden_states = encoder_hidden_states * attention_mask\n",
      "        # pass though first layer\n",
      "        shape=tf.shape(masked_encoder_hidden_states)[1]\n",
      "        spk_info=tf.repeat(spk_info,shape,1)\n",
      "        outputs = self.conv_layers_sequence(tf.concat([masked_encoder_hidden_states,spk_info],-1))\n",
      "        outputs = self.output_layer(outputs)\n",
      "        masked_outputs = outputs * attention_mask\n",
      "        return tf.squeeze(tf.nn.relu(masked_outputs), -1)  # make sure positive value.\n",
      "class TFFastSpeechLengthRegulator(tf.keras.layers.Layer):\n",
      "    \"\"\"FastSpeech lengthregulator module.\"\"\"\n",
      "    def __init__(self, config, **kwargs):\n",
      "        \"\"\"Init variables.\"\"\"\n",
      "        super().__init__(**kwargs)\n",
      "        self.config = config\n",
      "    def call(self, inputs, training=False):\n",
      "        \"\"\"Call logic.\n",
      "        Args:\n",
      "            1. encoder_hidden_states, Tensor (float32) shape [batch_size, length, hidden_size]\n",
      "            2. durations_gt, Tensor (float32/int32) shape [batch_size, length]\n",
      "        \"\"\"\n",
      "        encoder_hidden_states, durations_gt = inputs\n",
      "        outputs, encoder_masks = self._length_regulator(encoder_hidden_states, durations_gt)\n",
      "        return outputs, encoder_masks\n",
      "    def _length_regulator(self, encoder_hidden_states, durations_gt):\n",
      "        \"\"\"Length regulator logic.\"\"\"\n",
      "        sum_durations = tf.reduce_sum(durations_gt, axis=-1)  # [batch_size]\n",
      "        max_durations = tf.reduce_max(sum_durations)\n",
      "        input_shape = tf.shape(encoder_hidden_states)\n",
      "        batch_size = input_shape[0]\n",
      "        hidden_size = input_shape[-1]\n",
      "        # initialize output hidden states and encoder masking.\n",
      "        outputs = tf.zeros(shape=[0, max_durations, hidden_size], dtype=tf.float32)\n",
      "        encoder_masks = tf.zeros(shape=[0, max_durations], dtype=tf.int32)\n",
      "        def condition(i,\n",
      "                      batch_size,\n",
      "                      outputs,\n",
      "                      encoder_masks,\n",
      "                      encoder_hidden_states,\n",
      "                      durations_gt,\n",
      "                      max_durations):\n",
      "            return tf.less(i, batch_size)\n",
      "        def body(i,\n",
      "                 batch_size,\n",
      "                 outputs,\n",
      "                 encoder_masks,\n",
      "                 encoder_hidden_states,\n",
      "                 durations_gt,\n",
      "                 max_durations):\n",
      "            repeats = durations_gt[i]\n",
      "            real_length = tf.reduce_sum(repeats)\n",
      "            pad_size = max_durations - real_length\n",
      "            masks = tf.sequence_mask([real_length], max_durations, dtype=tf.int32)\n",
      "            repeat_encoder_hidden_states = tf.repeat(\n",
      "                encoder_hidden_states[i],\n",
      "                repeats=repeats,\n",
      "                axis=0\n",
      "            )\n",
      "            repeat_encoder_hidden_states = tf.expand_dims(\n",
      "                tf.pad(\n",
      "                    repeat_encoder_hidden_states, [[0, pad_size], [0, 0]]\n",
      "                ),\n",
      "                0)  # [1, max_durations, hidden_size]\n",
      "            outputs = tf.concat([outputs, repeat_encoder_hidden_states], axis=0)\n",
      "            encoder_masks = tf.concat([encoder_masks, masks], axis=0)\n",
      "            return [i + 1, batch_size, outputs, encoder_masks,\n",
      "                    encoder_hidden_states, durations_gt, max_durations]\n",
      "        # initialize iteration i.\n",
      "        i = tf.constant(0, dtype=tf.int32)\n",
      "        _, _, outputs, encoder_masks, _, _, _, = tf.while_loop(\n",
      "            condition,\n",
      "            body,\n",
      "            [i, batch_size, outputs, encoder_masks, encoder_hidden_states, durations_gt, max_durations],\n",
      "            shape_invariants=[i.get_shape(),\n",
      "                              batch_size.get_shape(),\n",
      "                              tf.TensorShape([None, None, self.config.hidden_size]),\n",
      "                              tf.TensorShape([None, None]),\n",
      "                              encoder_hidden_states.get_shape(),\n",
      "                              durations_gt.get_shape(),\n",
      "                              max_durations.get_shape()]\n",
      "        )\n",
      "        return outputs, encoder_masks\n",
      "class TFFastSpeech(tf.keras.Model):\n",
      "    \"\"\"TF Fastspeech module.\"\"\"\n",
      "    def __init__(self, config, **kwargs):\n",
      "        \"\"\"Init layers for fastspeech.\"\"\"\n",
      "        super().__init__(**kwargs)\n",
      "        self.embeddings = TFFastSpeechEmbeddings(config, name='embeddings')\n",
      "        self.encoder = TFFastSpeechEncoder(config, name='encoder')\n",
      "        self.duration_predictor = TFFastSpeechDurationPredictor(config, name='duration_predictor')\n",
      "        self.length_regulator = TFFastSpeechLengthRegulator(config, name='length_regulator')\n",
      "        self.decoder = TFFastSpeechDecoder(config, name='decoder')\n",
      "        self.mel_dense = tf.keras.layers.Dense(units=config.num_mels, name='mel_before')\n",
      "        self.postnet = TFPostnet(config=config, name='postnet')\n",
      "    def _build(self):\n",
      "        \"\"\"Dummy input for building model.\"\"\"\n",
      "        # fake inputs\n",
      "        input_ids = tf.convert_to_tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]], tf.int32)\n",
      "        attention_mask = tf.convert_to_tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], tf.int32)\n",
      "        speaker_ids = tf.convert_to_tensor([[0]], tf.int32)\n",
      "        duration_gts = tf.convert_to_tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], tf.int32)\n",
      "        self(input_ids, attention_mask, speaker_ids, duration_gts)\n",
      "    def call(self,\n",
      "             input_ids,\n",
      "             attention_mask,\n",
      "             speaker_ids,\n",
      "             duration_gts,\n",
      "             training=False):\n",
      "        \"\"\"Call logic.\"\"\"\n",
      "        embedding_output,speaker_info = self.embeddings([input_ids, speaker_ids], training=training)\n",
      "        encoder_output = self.encoder([embedding_output, attention_mask], training=training)\n",
      "        last_encoder_hidden_states = encoder_output[0]\n",
      "        # duration predictor, here use last_encoder_hidden_states, u can use more hidden_states layers\n",
      "        # rather than just use last_hidden_states of encoder for duration_predictor.\n",
      "        duration_outputs = self.duration_predictor([last_encoder_hidden_states, attention_mask,speaker_info])  # [batch_size, length]\n",
      "        length_regulator_outputs, encoder_masks = self.length_regulator([\n",
      "            last_encoder_hidden_states, duration_gts], training=training)\n",
      "        # create decoder positional embedding\n",
      "        decoder_pos = tf.range(1, tf.shape(length_regulator_outputs)[1] + 1, dtype=tf.int32)\n",
      "        masked_decoder_pos = tf.expand_dims(decoder_pos, 0) * encoder_masks\n",
      "        decoder_output = self.decoder(\n",
      "            [length_regulator_outputs, speaker_info, encoder_masks, masked_decoder_pos], training=training)\n",
      "        last_decoder_hidden_states = decoder_output[0]\n",
      "        # here u can use sum or concat more than 1 hidden states layers from decoder.\n",
      "        mel_before = self.mel_dense(last_decoder_hidden_states)\n",
      "        mel_after = self.postnet([mel_before, encoder_masks], training=training) + mel_before\n",
      "        outputs = (mel_before, mel_after, duration_outputs)\n",
      "        return outputs\n",
      "    @tf.function(experimental_relax_shapes=True )\n",
      "    def inference(self,\n",
      "                  input_ids,\n",
      "                  attention_mask,\n",
      "                  speaker_ids,\n",
      "                  duration_gts=None,\n",
      "                  speed_ratios=1.0):\n",
      "        \"\"\"Call logic.\"\"\"\n",
      "        embedding_output,speaker_info = self.embeddings([input_ids, speaker_ids], training=False)\n",
      "        encoder_output = self.encoder([embedding_output, attention_mask], training=False)\n",
      "        last_encoder_hidden_states = encoder_output[0]\n",
      "        # print(encoder_output,last_encoder_hidden_states)\n",
      "        # duration predictor, here use last_encoder_hidden_states, u can use more hidden_states layers\n",
      "        # rather than just use last_hidden_states of encoder for duration_predictor.\n",
      "        duration_outputs = self.duration_predictor([last_encoder_hidden_states, attention_mask,speaker_info])  # [batch_size, length]\n",
      "        if speed_ratios is None:\n",
      "            speed_ratios = tf.convert_to_tensor(np.array([1.0]), dtype=tf.float32)\n",
      "        duration_outputs = tf.cast(tf.math.round(duration_outputs * speed_ratios), tf.int32)\n",
      "        if duration_gts is not None:\n",
      "            duration_outputs = duration_gts\n",
      "        length_regulator_outputs, encoder_masks = self.length_regulator([\n",
      "            last_encoder_hidden_states, duration_outputs], training=False)\n",
      "        # create decoder positional embedding\n",
      "        decoder_pos = tf.range(1, tf.shape(length_regulator_outputs)[1] + 1, dtype=tf.int32)\n",
      "        masked_decoder_pos = tf.expand_dims(decoder_pos, 0) * encoder_masks\n",
      "        decoder_output = self.decoder(\n",
      "            [length_regulator_outputs, speaker_info, encoder_masks, masked_decoder_pos], training=False)\n",
      "        last_decoder_hidden_states = decoder_output[0]\n",
      "        # here u can use sum or concat more than 1 hidden states layers from decoder.\n",
      "        mel_before = self.mel_dense(last_decoder_hidden_states)\n",
      "        mel_after = self.postnet([mel_before, encoder_masks], training=False)+ mel_before\n",
      "        outputs = (mel_before, mel_after, duration_outputs)\n",
      "        return outputs\n"
     ]
    }
   ],
   "source": [
    "print(getSequentialModel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}